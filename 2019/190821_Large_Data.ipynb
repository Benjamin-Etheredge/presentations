{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "190821_Large_Data.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HSV-AI/presentations/blob/master/2019/190821_Large_Data.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VGyzz9iJPdyM",
        "colab_type": "text"
      },
      "source": [
        "# Huntsville AI"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OWms3K5Ncu_J",
        "colab_type": "text"
      },
      "source": [
        "## Agenda\n",
        "\n",
        "* Intro & Welcome\n",
        "* Project Updates\n",
        "** T-Shirts\n",
        "* Deep Learning with Large Data Sets\n",
        "* Planning for Fall\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E3O0xMwF_QqS",
        "colab_type": "text"
      },
      "source": [
        "## Deep Learning with Large Data Sets\n",
        "\n",
        "There are times when you will need to be able to train a neural network with a dataset that is larger than can fit into the available memory of a machine.\n",
        "\n",
        "So far, the first time I have encountered this issue is with an RNN that takes a sequence of input data.\n",
        "\n",
        "For example, the project that I'm working with Radar and Satellite data has the following input structure:\n",
        "\n",
        "```\n",
        "year/month/day/\n",
        "    radar/\n",
        "        images (720x1832 float64) ~= 10MB (average ~150-300 scans per day)\n",
        "    sat/\n",
        "        band/\n",
        "            images (each band scanned every 5 minutes)\n",
        "    rain.csv\n",
        "```\n",
        "\n",
        "Assuming that the sat images are similar to the radar images, that gives us around 10GB of data per day.\n",
        "\n",
        "The approach with Keras is to use a Data Generator to load the data incrementally when training the model.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iU54eLuCQFOi",
        "colab_type": "text"
      },
      "source": [
        "## References:\n",
        "\n",
        "[ImageDataGenerator](https://medium.com/datadriveninvestor/keras-imagedatagenerator-methods-an-easy-guide-550ecd3c0a92)  \\[[source](https://github.com/keras-team/keras/blob/master/keras/preprocessing/image.py#L233)\\]\n",
        "\n",
        "[Multiprocess ImageDataGenerator](https://github.com/stratospark/keras-multiprocess-image-data-generator)\n",
        "\n",
        "[Stanford Tutorial](https://stanford.edu/~shervine/blog/keras-how-to-generate-data-on-the-fly)\n",
        "\n",
        "[Loading Large Data](https://medium.com/datadriveninvestor/keras-training-on-large-datasets-3e9d9dbc09d4)\n",
        "\n",
        "[More Loading Large Datasets](https://machinelearningmastery.com/how-to-load-large-datasets-from-directories-for-deep-learning-with-keras/)\n",
        "\n",
        "[Keras Feature extraction on large data](https://www.pyimagesearch.com/2019/05/27/keras-feature-extraction-on-large-datasets-with-deep-learning/)\n",
        "\n",
        "[Keras Fit vs Fit_Generator](https://www.pyimagesearch.com/2018/12/24/how-to-use-keras-fit-and-fit_generator-a-hands-on-tutorial/)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3eHj4mBzUIQ3",
        "colab_type": "text"
      },
      "source": [
        "## Keras Example\n",
        "\n",
        "Here's an example from the Keras codebase that uses the ImageDataGenerator. You can think of this as a different approach that takes a given set of images and increases the data size by applying transformations to create new images based off of existing data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6wgJo8fa-TSJ",
        "colab_type": "code",
        "outputId": "a82576fd-c45c-4075-8da8-7c3f48cdb92f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "'''\n",
        "#Train a simple deep CNN on the CIFAR10 small images dataset.\n",
        "It gets to 75% validation accuracy in 25 epochs, and 79% after 50 epochs.\n",
        "(it's still underfitting at that point, though).\n",
        "'''\n",
        "\n",
        "from __future__ import print_function\n",
        "import keras\n",
        "from keras.datasets import cifar10\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Activation, Flatten\n",
        "from keras.layers import Conv2D, MaxPooling2D\n",
        "import os\n",
        "\n",
        "from math import ceil\n",
        "\n",
        "batch_size = 32\n",
        "num_classes = 10\n",
        "epochs = 100\n",
        "data_augmentation = True\n",
        "num_predictions = 20\n",
        "save_dir = os.path.join(os.getcwd(), 'saved_models')\n",
        "model_name = 'keras_cifar10_trained_model.h5'\n",
        "\n",
        "# The data, split between train and test sets:\n",
        "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
        "print('x_train shape:', x_train.shape)\n",
        "print(x_train.shape[0], 'train samples')\n",
        "print(x_test.shape[0], 'test samples')\n",
        "\n",
        "\n",
        "n_points = len(x_train)\n",
        "\n",
        "steps_per_epoch = ceil(n_points / batch_size)\n",
        "\n",
        "\n",
        "# Convert class vectors to binary class matrices.\n",
        "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
        "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Conv2D(32, (3, 3), padding='same',\n",
        "                 input_shape=x_train.shape[1:]))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Conv2D(32, (3, 3)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Dropout(0.25))\n",
        "\n",
        "model.add(Conv2D(64, (3, 3), padding='same'))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Conv2D(64, (3, 3)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Dropout(0.25))\n",
        "\n",
        "model.add(Flatten())\n",
        "model.add(Dense(512))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(num_classes))\n",
        "model.add(Activation('softmax'))\n",
        "\n",
        "# initiate RMSprop optimizer\n",
        "opt = keras.optimizers.rmsprop(lr=0.0001, decay=1e-6)\n",
        "\n",
        "# Let's train the model using RMSprop\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=opt,\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "x_train = x_train.astype('float32')\n",
        "x_test = x_test.astype('float32')\n",
        "x_train /= 255\n",
        "x_test /= 255\n",
        "\n",
        "if not data_augmentation:\n",
        "    print('Not using data augmentation.')\n",
        "    model.fit(x_train, y_train,\n",
        "              batch_size=batch_size,\n",
        "              epochs=epochs,\n",
        "              validation_data=(x_test, y_test),\n",
        "              shuffle=True)\n",
        "else:\n",
        "    print('Using real-time data augmentation.')\n",
        "    # This will do preprocessing and realtime data augmentation:\n",
        "    datagen = ImageDataGenerator(\n",
        "        featurewise_center=False,  # set input mean to 0 over the dataset\n",
        "        samplewise_center=False,  # set each sample mean to 0\n",
        "        featurewise_std_normalization=False,  # divide inputs by std of the dataset\n",
        "        samplewise_std_normalization=False,  # divide each input by its std\n",
        "        zca_whitening=False,  # apply ZCA whitening\n",
        "        zca_epsilon=1e-06,  # epsilon for ZCA whitening\n",
        "        rotation_range=0,  # randomly rotate images in the range (degrees, 0 to 180)\n",
        "        # randomly shift images horizontally (fraction of total width)\n",
        "        width_shift_range=0.1,\n",
        "        # randomly shift images vertically (fraction of total height)\n",
        "        height_shift_range=0.1,\n",
        "        shear_range=0.,  # set range for random shear\n",
        "        zoom_range=0.,  # set range for random zoom\n",
        "        channel_shift_range=0.,  # set range for random channel shifts\n",
        "        # set mode for filling points outside the input boundaries\n",
        "        fill_mode='nearest',\n",
        "        cval=0.,  # value used for fill_mode = \"constant\"\n",
        "        horizontal_flip=True,  # randomly flip images\n",
        "        vertical_flip=False,  # randomly flip images\n",
        "        # set rescaling factor (applied before any other transformation)\n",
        "        rescale=None,\n",
        "        # set function that will be applied on each input\n",
        "        preprocessing_function=None,\n",
        "        # image data format, either \"channels_first\" or \"channels_last\"\n",
        "        data_format=None,\n",
        "        # fraction of images reserved for validation (strictly between 0 and 1)\n",
        "        validation_split=0.0)\n",
        "\n",
        "    # Compute quantities required for feature-wise normalization\n",
        "    # (std, mean, and principal components if ZCA whitening is applied).\n",
        "    datagen.fit(x_train)\n",
        "\n",
        "    # Fit the model on the batches generated by datagen.flow().\n",
        "    model.fit_generator(datagen.flow(x_train, y_train,\n",
        "                                     batch_size=batch_size),\n",
        "                        epochs=epochs,\n",
        "                        steps_per_epoch=steps_per_epoch,\n",
        "                        validation_data=(x_test, y_test),\n",
        "                        workers=4)\n",
        "\n",
        "# Save model and weights\n",
        "if not os.path.isdir(save_dir):\n",
        "    os.makedirs(save_dir)\n",
        "model_path = os.path.join(save_dir, model_name)\n",
        "model.save(model_path)\n",
        "print('Saved trained model at %s ' % model_path)\n",
        "\n",
        "# Score trained model.\n",
        "scores = model.evaluate(x_test, y_test, verbose=1)\n",
        "print('Test loss:', scores[0])\n",
        "print('Test accuracy:', scores[1])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "x_train shape: (50000, 32, 32, 3)\n",
            "50000 train samples\n",
            "10000 test samples\n",
            "Using real-time data augmentation.\n",
            "Epoch 1/100\n",
            "1563/1563 [==============================] - 31s 20ms/step - loss: 1.8888 - acc: 0.3081 - val_loss: 1.5796 - val_acc: 0.4246\n",
            "Epoch 2/100\n",
            "1563/1563 [==============================] - 30s 19ms/step - loss: 1.5950 - acc: 0.4155 - val_loss: 1.4498 - val_acc: 0.4679\n",
            "Epoch 3/100\n",
            "1563/1563 [==============================] - 30s 19ms/step - loss: 1.4648 - acc: 0.4669 - val_loss: 1.3281 - val_acc: 0.5237\n",
            "Epoch 4/100\n",
            "1563/1563 [==============================] - 31s 20ms/step - loss: 1.3778 - acc: 0.5040 - val_loss: 1.2068 - val_acc: 0.5666\n",
            "Epoch 5/100\n",
            "1563/1563 [==============================] - 30s 20ms/step - loss: 1.3005 - acc: 0.5367 - val_loss: 1.1448 - val_acc: 0.5890\n",
            "Epoch 6/100\n",
            "1563/1563 [==============================] - 31s 20ms/step - loss: 1.2436 - acc: 0.5574 - val_loss: 1.2455 - val_acc: 0.5662\n",
            "Epoch 7/100\n",
            "1563/1563 [==============================] - 31s 20ms/step - loss: 1.1940 - acc: 0.5768 - val_loss: 1.1144 - val_acc: 0.6064\n",
            "Epoch 8/100\n",
            "1563/1563 [==============================] - 31s 20ms/step - loss: 1.1542 - acc: 0.5921 - val_loss: 1.0732 - val_acc: 0.6178\n",
            "Epoch 9/100\n",
            "1563/1563 [==============================] - 30s 19ms/step - loss: 1.1181 - acc: 0.6051 - val_loss: 1.1320 - val_acc: 0.6046\n",
            "Epoch 10/100\n",
            "1563/1563 [==============================] - 30s 19ms/step - loss: 1.0832 - acc: 0.6184 - val_loss: 0.9794 - val_acc: 0.6564\n",
            "Epoch 11/100\n",
            "1563/1563 [==============================] - 30s 19ms/step - loss: 1.0646 - acc: 0.6256 - val_loss: 0.9268 - val_acc: 0.6750\n",
            "Epoch 12/100\n",
            "1563/1563 [==============================] - 30s 19ms/step - loss: 1.0379 - acc: 0.6340 - val_loss: 0.9803 - val_acc: 0.6574\n",
            "Epoch 13/100\n",
            "1563/1563 [==============================] - 30s 19ms/step - loss: 1.0186 - acc: 0.6419 - val_loss: 0.8696 - val_acc: 0.6991\n",
            "Epoch 14/100\n",
            "1563/1563 [==============================] - 31s 20ms/step - loss: 0.9955 - acc: 0.6520 - val_loss: 0.8852 - val_acc: 0.6900\n",
            "Epoch 15/100\n",
            "1563/1563 [==============================] - 31s 20ms/step - loss: 0.9792 - acc: 0.6568 - val_loss: 0.8849 - val_acc: 0.6883\n",
            "Epoch 16/100\n",
            "1563/1563 [==============================] - 31s 20ms/step - loss: 0.9651 - acc: 0.6616 - val_loss: 0.8277 - val_acc: 0.7109\n",
            "Epoch 17/100\n",
            "1563/1563 [==============================] - 31s 20ms/step - loss: 0.9512 - acc: 0.6670 - val_loss: 0.8411 - val_acc: 0.7088\n",
            "Epoch 18/100\n",
            "1563/1563 [==============================] - 31s 20ms/step - loss: 0.9386 - acc: 0.6735 - val_loss: 0.8132 - val_acc: 0.7181\n",
            "Epoch 19/100\n",
            "1563/1563 [==============================] - 31s 20ms/step - loss: 0.9312 - acc: 0.6777 - val_loss: 0.8275 - val_acc: 0.7168\n",
            "Epoch 20/100\n",
            "1563/1563 [==============================] - 31s 20ms/step - loss: 0.9189 - acc: 0.6800 - val_loss: 0.8346 - val_acc: 0.7093\n",
            "Epoch 21/100\n",
            "1563/1563 [==============================] - 31s 20ms/step - loss: 0.9063 - acc: 0.6850 - val_loss: 0.7739 - val_acc: 0.7340\n",
            "Epoch 22/100\n",
            "1563/1563 [==============================] - 31s 20ms/step - loss: 0.9012 - acc: 0.6880 - val_loss: 0.7881 - val_acc: 0.7363\n",
            "Epoch 23/100\n",
            "1563/1563 [==============================] - 31s 20ms/step - loss: 0.8904 - acc: 0.6927 - val_loss: 0.8672 - val_acc: 0.7048\n",
            "Epoch 24/100\n",
            "1563/1563 [==============================] - 31s 20ms/step - loss: 0.8865 - acc: 0.6928 - val_loss: 0.8272 - val_acc: 0.7248\n",
            "Epoch 25/100\n",
            "1563/1563 [==============================] - 31s 20ms/step - loss: 0.8817 - acc: 0.6959 - val_loss: 0.8464 - val_acc: 0.7149\n",
            "Epoch 26/100\n",
            "1563/1563 [==============================] - 30s 20ms/step - loss: 0.8727 - acc: 0.6987 - val_loss: 0.8260 - val_acc: 0.7244\n",
            "Epoch 27/100\n",
            "1563/1563 [==============================] - 31s 20ms/step - loss: 0.8726 - acc: 0.7000 - val_loss: 0.7757 - val_acc: 0.7381\n",
            "Epoch 28/100\n",
            "1563/1563 [==============================] - 31s 20ms/step - loss: 0.8650 - acc: 0.7016 - val_loss: 0.7845 - val_acc: 0.7307\n",
            "Epoch 29/100\n",
            "1563/1563 [==============================] - 31s 20ms/step - loss: 0.8572 - acc: 0.7041 - val_loss: 0.7616 - val_acc: 0.7401\n",
            "Epoch 30/100\n",
            "1563/1563 [==============================] - 31s 20ms/step - loss: 0.8529 - acc: 0.7066 - val_loss: 0.7692 - val_acc: 0.7424\n",
            "Epoch 31/100\n",
            "1563/1563 [==============================] - 30s 20ms/step - loss: 0.8488 - acc: 0.7080 - val_loss: 0.7834 - val_acc: 0.7339\n",
            "Epoch 32/100\n",
            "1563/1563 [==============================] - 31s 20ms/step - loss: 0.8449 - acc: 0.7096 - val_loss: 0.7400 - val_acc: 0.7485\n",
            "Epoch 33/100\n",
            "1563/1563 [==============================] - 31s 20ms/step - loss: 0.8411 - acc: 0.7127 - val_loss: 0.7679 - val_acc: 0.7426\n",
            "Epoch 34/100\n",
            "1563/1563 [==============================] - 31s 20ms/step - loss: 0.8354 - acc: 0.7124 - val_loss: 0.7072 - val_acc: 0.7604\n",
            "Epoch 35/100\n",
            "1563/1563 [==============================] - 31s 20ms/step - loss: 0.8359 - acc: 0.7159 - val_loss: 0.7698 - val_acc: 0.7367\n",
            "Epoch 36/100\n",
            "1563/1563 [==============================] - 31s 20ms/step - loss: 0.8284 - acc: 0.7185 - val_loss: 0.7209 - val_acc: 0.7565\n",
            "Epoch 37/100\n",
            "1563/1563 [==============================] - 31s 20ms/step - loss: 0.8296 - acc: 0.7167 - val_loss: 0.7725 - val_acc: 0.7394\n",
            "Epoch 38/100\n",
            "1563/1563 [==============================] - 31s 20ms/step - loss: 0.8280 - acc: 0.7181 - val_loss: 0.7660 - val_acc: 0.7413\n",
            "Epoch 39/100\n",
            "1563/1563 [==============================] - 31s 20ms/step - loss: 0.8261 - acc: 0.7185 - val_loss: 0.7471 - val_acc: 0.7467\n",
            "Epoch 40/100\n",
            "1563/1563 [==============================] - 31s 20ms/step - loss: 0.8197 - acc: 0.7214 - val_loss: 0.7493 - val_acc: 0.7456\n",
            "Epoch 41/100\n",
            "1563/1563 [==============================] - 31s 20ms/step - loss: 0.8157 - acc: 0.7231 - val_loss: 0.7273 - val_acc: 0.7534\n",
            "Epoch 42/100\n",
            "1563/1563 [==============================] - 30s 19ms/step - loss: 0.8127 - acc: 0.7234 - val_loss: 0.7033 - val_acc: 0.7663\n",
            "Epoch 43/100\n",
            "1563/1563 [==============================] - 31s 20ms/step - loss: 0.8236 - acc: 0.7195 - val_loss: 0.7225 - val_acc: 0.7541\n",
            "Epoch 44/100\n",
            "1563/1563 [==============================] - 31s 20ms/step - loss: 0.8154 - acc: 0.7222 - val_loss: 0.7475 - val_acc: 0.7433\n",
            "Epoch 45/100\n",
            "1563/1563 [==============================] - 31s 20ms/step - loss: 0.8135 - acc: 0.7252 - val_loss: 0.7172 - val_acc: 0.7622\n",
            "Epoch 46/100\n",
            "1563/1563 [==============================] - 31s 20ms/step - loss: 0.8079 - acc: 0.7243 - val_loss: 0.7429 - val_acc: 0.7535\n",
            "Epoch 47/100\n",
            "1563/1563 [==============================] - 31s 20ms/step - loss: 0.8088 - acc: 0.7264 - val_loss: 0.7987 - val_acc: 0.7318\n",
            "Epoch 48/100\n",
            "1563/1563 [==============================] - 31s 20ms/step - loss: 0.8098 - acc: 0.7266 - val_loss: 0.7725 - val_acc: 0.7437\n",
            "Epoch 49/100\n",
            "1563/1563 [==============================] - 31s 20ms/step - loss: 0.8017 - acc: 0.7278 - val_loss: 0.7392 - val_acc: 0.7485\n",
            "Epoch 50/100\n",
            "1563/1563 [==============================] - 31s 20ms/step - loss: 0.8020 - acc: 0.7286 - val_loss: 0.6994 - val_acc: 0.7683\n",
            "Epoch 51/100\n",
            "1563/1563 [==============================] - 31s 20ms/step - loss: 0.7983 - acc: 0.7329 - val_loss: 0.7315 - val_acc: 0.7562\n",
            "Epoch 52/100\n",
            "1563/1563 [==============================] - 30s 19ms/step - loss: 0.8041 - acc: 0.7303 - val_loss: 0.6834 - val_acc: 0.7699\n",
            "Epoch 53/100\n",
            "1563/1563 [==============================] - 30s 19ms/step - loss: 0.8016 - acc: 0.7289 - val_loss: 0.7256 - val_acc: 0.7526\n",
            "Epoch 54/100\n",
            "1563/1563 [==============================] - 31s 20ms/step - loss: 0.7971 - acc: 0.7329 - val_loss: 0.7188 - val_acc: 0.7530\n",
            "Epoch 55/100\n",
            "1563/1563 [==============================] - 30s 19ms/step - loss: 0.7951 - acc: 0.7304 - val_loss: 0.6851 - val_acc: 0.7671\n",
            "Epoch 56/100\n",
            "1563/1563 [==============================] - 30s 19ms/step - loss: 0.7999 - acc: 0.7319 - val_loss: 0.7218 - val_acc: 0.7650\n",
            "Epoch 57/100\n",
            "1563/1563 [==============================] - 31s 20ms/step - loss: 0.7942 - acc: 0.7322 - val_loss: 0.6609 - val_acc: 0.7779\n",
            "Epoch 58/100\n",
            "1563/1563 [==============================] - 31s 20ms/step - loss: 0.7915 - acc: 0.7351 - val_loss: 0.7780 - val_acc: 0.7411\n",
            "Epoch 59/100\n",
            "1563/1563 [==============================] - 31s 20ms/step - loss: 0.7861 - acc: 0.7344 - val_loss: 0.7096 - val_acc: 0.7640\n",
            "Epoch 60/100\n",
            "1563/1563 [==============================] - 31s 20ms/step - loss: 0.7917 - acc: 0.7349 - val_loss: 0.7629 - val_acc: 0.7393\n",
            "Epoch 61/100\n",
            "1563/1563 [==============================] - 31s 20ms/step - loss: 0.7902 - acc: 0.7356 - val_loss: 0.7593 - val_acc: 0.7501\n",
            "Epoch 62/100\n",
            "1563/1563 [==============================] - 31s 20ms/step - loss: 0.7911 - acc: 0.7340 - val_loss: 0.6896 - val_acc: 0.7677\n",
            "Epoch 63/100\n",
            "1563/1563 [==============================] - 31s 20ms/step - loss: 0.7861 - acc: 0.7366 - val_loss: 0.7194 - val_acc: 0.7589\n",
            "Epoch 64/100\n",
            "1563/1563 [==============================] - 31s 20ms/step - loss: 0.7971 - acc: 0.7321 - val_loss: 0.7059 - val_acc: 0.7662\n",
            "Epoch 65/100\n",
            "1563/1563 [==============================] - 31s 20ms/step - loss: 0.7895 - acc: 0.7354 - val_loss: 0.7501 - val_acc: 0.7552\n",
            "Epoch 66/100\n",
            "1563/1563 [==============================] - 30s 19ms/step - loss: 0.7925 - acc: 0.7352 - val_loss: 0.7366 - val_acc: 0.7697\n",
            "Epoch 67/100\n",
            "1563/1563 [==============================] - 30s 19ms/step - loss: 0.7888 - acc: 0.7344 - val_loss: 0.7504 - val_acc: 0.7464\n",
            "Epoch 68/100\n",
            "1563/1563 [==============================] - 30s 19ms/step - loss: 0.7938 - acc: 0.7363 - val_loss: 0.7282 - val_acc: 0.7511\n",
            "Epoch 69/100\n",
            "1563/1563 [==============================] - 30s 19ms/step - loss: 0.7865 - acc: 0.7367 - val_loss: 0.7112 - val_acc: 0.7607\n",
            "Epoch 70/100\n",
            "1563/1563 [==============================] - 30s 19ms/step - loss: 0.7837 - acc: 0.7381 - val_loss: 0.6503 - val_acc: 0.7866\n",
            "Epoch 71/100\n",
            "1563/1563 [==============================] - 30s 19ms/step - loss: 0.7817 - acc: 0.7386 - val_loss: 0.7183 - val_acc: 0.7593\n",
            "Epoch 72/100\n",
            "1563/1563 [==============================] - 30s 19ms/step - loss: 0.7802 - acc: 0.7376 - val_loss: 0.6615 - val_acc: 0.7807\n",
            "Epoch 73/100\n",
            "1563/1563 [==============================] - 30s 19ms/step - loss: 0.7868 - acc: 0.7361 - val_loss: 0.6982 - val_acc: 0.7794\n",
            "Epoch 74/100\n",
            "1563/1563 [==============================] - 30s 19ms/step - loss: 0.7922 - acc: 0.7359 - val_loss: 0.7361 - val_acc: 0.7658\n",
            "Epoch 75/100\n",
            "1563/1563 [==============================] - 30s 19ms/step - loss: 0.7832 - acc: 0.7380 - val_loss: 0.8591 - val_acc: 0.7089\n",
            "Epoch 76/100\n",
            "1563/1563 [==============================] - 30s 19ms/step - loss: 0.7860 - acc: 0.7374 - val_loss: 0.7909 - val_acc: 0.7330\n",
            "Epoch 77/100\n",
            "1563/1563 [==============================] - 30s 19ms/step - loss: 0.7827 - acc: 0.7379 - val_loss: 0.6997 - val_acc: 0.7688\n",
            "Epoch 78/100\n",
            "1563/1563 [==============================] - 30s 19ms/step - loss: 0.7847 - acc: 0.7370 - val_loss: 0.7417 - val_acc: 0.7557\n",
            "Epoch 79/100\n",
            "1563/1563 [==============================] - 30s 19ms/step - loss: 0.7887 - acc: 0.7355 - val_loss: 0.6754 - val_acc: 0.7756\n",
            "Epoch 80/100\n",
            "1563/1563 [==============================] - 30s 19ms/step - loss: 0.7885 - acc: 0.7364 - val_loss: 0.9642 - val_acc: 0.6993\n",
            "Epoch 81/100\n",
            "1563/1563 [==============================] - 30s 19ms/step - loss: 0.7867 - acc: 0.7359 - val_loss: 0.7186 - val_acc: 0.7626\n",
            "Epoch 82/100\n",
            "1563/1563 [==============================] - 30s 19ms/step - loss: 0.7851 - acc: 0.7365 - val_loss: 0.7174 - val_acc: 0.7607\n",
            "Epoch 83/100\n",
            "1563/1563 [==============================] - 30s 19ms/step - loss: 0.7873 - acc: 0.7379 - val_loss: 0.7089 - val_acc: 0.7691\n",
            "Epoch 84/100\n",
            "1563/1563 [==============================] - 30s 19ms/step - loss: 0.7902 - acc: 0.7382 - val_loss: 0.6812 - val_acc: 0.7754\n",
            "Epoch 85/100\n",
            "1563/1563 [==============================] - 30s 19ms/step - loss: 0.7945 - acc: 0.7357 - val_loss: 0.7623 - val_acc: 0.7511\n",
            "Epoch 86/100\n",
            "1563/1563 [==============================] - 30s 19ms/step - loss: 0.7879 - acc: 0.7381 - val_loss: 0.7171 - val_acc: 0.7626\n",
            "Epoch 87/100\n",
            "1563/1563 [==============================] - 30s 19ms/step - loss: 0.7837 - acc: 0.7377 - val_loss: 0.7788 - val_acc: 0.7472\n",
            "Epoch 88/100\n",
            "1563/1563 [==============================] - 30s 19ms/step - loss: 0.7922 - acc: 0.7391 - val_loss: 0.7794 - val_acc: 0.7401\n",
            "Epoch 89/100\n",
            "1563/1563 [==============================] - 30s 19ms/step - loss: 0.7923 - acc: 0.7354 - val_loss: 0.7119 - val_acc: 0.7699\n",
            "Epoch 90/100\n",
            "1563/1563 [==============================] - 30s 19ms/step - loss: 0.7950 - acc: 0.7369 - val_loss: 0.6720 - val_acc: 0.7783\n",
            "Epoch 91/100\n",
            "1563/1563 [==============================] - 30s 19ms/step - loss: 0.7894 - acc: 0.7384 - val_loss: 0.7742 - val_acc: 0.7440\n",
            "Epoch 92/100\n",
            "1563/1563 [==============================] - 30s 19ms/step - loss: 0.7936 - acc: 0.7369 - val_loss: 0.6867 - val_acc: 0.7800\n",
            "Epoch 93/100\n",
            "1563/1563 [==============================] - 30s 19ms/step - loss: 0.7938 - acc: 0.7380 - val_loss: 0.6889 - val_acc: 0.7784\n",
            "Epoch 94/100\n",
            "1563/1563 [==============================] - 30s 19ms/step - loss: 0.7900 - acc: 0.7376 - val_loss: 0.6880 - val_acc: 0.7713\n",
            "Epoch 95/100\n",
            "1563/1563 [==============================] - 30s 19ms/step - loss: 0.7957 - acc: 0.7362 - val_loss: 0.6850 - val_acc: 0.7807\n",
            "Epoch 96/100\n",
            "1563/1563 [==============================] - 30s 19ms/step - loss: 0.7966 - acc: 0.7398 - val_loss: 0.7368 - val_acc: 0.7574\n",
            "Epoch 97/100\n",
            "1563/1563 [==============================] - 30s 19ms/step - loss: 0.7932 - acc: 0.7360 - val_loss: 0.7736 - val_acc: 0.7569\n",
            "Epoch 98/100\n",
            "1563/1563 [==============================] - 30s 19ms/step - loss: 0.7947 - acc: 0.7384 - val_loss: 0.7938 - val_acc: 0.7397\n",
            "Epoch 99/100\n",
            "1563/1563 [==============================] - 30s 19ms/step - loss: 0.7981 - acc: 0.7362 - val_loss: 0.7807 - val_acc: 0.7329\n",
            "Epoch 100/100\n",
            "1563/1563 [==============================] - 30s 19ms/step - loss: 0.7956 - acc: 0.7385 - val_loss: 0.7134 - val_acc: 0.7646\n",
            "Saved trained model at /content/saved_models/keras_cifar10_trained_model.h5 \n",
            "10000/10000 [==============================] - 1s 88us/step\n",
            "Test loss: 0.7133953602313995\n",
            "Test accuracy: 0.7646\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}